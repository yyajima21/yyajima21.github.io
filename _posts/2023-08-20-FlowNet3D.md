---
title: 'Summary of "FlowNet3D: Learning Scene Flow in 3D Point Clouds"'
date: 2023-08-20
last_modified_at: 2023-08-20
permalink: /posts/2023/08/FlowNet3D/
tags:
  - FlowNet3D
  - Computer Vision
---

<p align="center">
<img src="images/blogs/flownet3d/flownet3d_intro.png" width="100%" />
   <br>
   <figcaption>

   Figure 1 from [FlowNet3D: Learning Scene Flow in 3D Point Clouds by Liu et al](https://arxiv.org/pdf/1806.01411.pdf).
   
   </figcaption>
</p>

## Motivations
In this post, I will explain the paper titled "FlowNet3D: Learning Scene Flow in 3D Point Clouds (CVPR2018)". Understanding dynamic object motions and behavior predictions in 3D space are important for many robotics applications such as self-driving cars and UAVs. One way to understand the object motion is to learn features from motion cues of points using deep learning model. However, previous scene flow work often relies on 2D representation such as image optical flow and is not designed for processing 3D points. 

Predicting the behavior of object motions from 3D point cloud is challenging due to its irregular format of the point cloud data. Unlike image data, point cloud doesn't have a specific order, and the method needs to take acount translation and rotation invariance of the input data. This paper shows a new method to predict object scene flows directly from two consecutive point cloud frames using end-to-end deep learning model.

## Important Notes
All images and graphs are reproduced from the flownet3d paper and the copyright belongs to the authors or the organization that publisdhed their paper. I used a key figure from the paper under the fair use clause of copyright law.

## Key Contributions
* Applies an end-to-end deep learning model that takes a pair of consecutive point cloud data and predict translational flow vectors
* Develops two learning layers (flow embeddings layer and set upconv layer) for learning correlation of two consecutive point cloud and propagating learned features from one set of points to the other
* Provide a 3D scene flow result using read LiDAR scans from KITTI dataset

## FlowNet3D Architecture
![FlowNet3D](/images/blogs/flownet3d/flownet3d_point_cloud_processing.png)
*Figure 2 from [FlowNet3D: Learning Scene Flow in 3D Point Clouds](https://arxiv.org/pdf/1806.01411.pdf) by Liu et al.*
<br /><br />
In this section, I will describe each network modules defined as point feature learning (set conv layer), point mixture (flow embedding layer), and flow refinement (set upconv layer). The FlowNet3D is consist of four set conv layers, one flow embedding layers and four set up conv layers. and a final linear flow regression layers. The last layer predicts a scene flow in 3D. 

### Assumptions
Before explaining the architecture, here are some assumptions made by the authors. Inputs to the deep learning model are two point clouds (not necessarily have equal number of points) defined as follows in XYZ coordinates.

$ P = \lbrace x_{i}|i = 1, ..., n_{1} \rbrace $ and $ \lbrace Q = y_{j}|j = 1, ..., n_{2} \rbrace, x_{i},y_{j}\in \mathbb{R^{3}}$ between $P$ and $Q$

The translational point motion vector is defined as $d_{i} = x_{i}^{'} - x_{i}$
and for all points are defined as follows $D = \lbrace d_{i}|i=1,...,n_{1}\rbrace$


### Hierarchical Point Cloud Feature Learning with Set Conv Layer
In the first modules, [PointNet++](https://arxiv.org/abs/1706.02413) architecture is used to extract key features from unordered set of input point cloud data. The network is a translation-invariant meaning that the unordered points must be invariance under the translation transformation. This means that the network must not change its prediction by translating input points.

The set conv layer takes $n$ input points that have $p_{i}=\lbrace x_i,f_i\rbrace$, where the $x_i$ represents its xyz position and $f_i$ represents its feature. During this step, the layer samples $n^{'}$ regions from the input points with farthest point sampling, then it extracts its local feature with the following symmetric functions defined below. The layer outputs the $n$ sab-sampled points with updated xyz position and features defined as $p_{j}^{'} = \lbrace x_{j}^{'}, f_{j}^{'} \rbrace$ 

$$ f_{j}^{'} = \underset{\{j | \lVert x_{i} - x_{i}^{'}\rVert \le r\}}{MAX} \{h(f_{i}, x_{i} - x_{j}^{'}) \} \text{, where h is a non-linear function (multi-layer perceptron)} $$

### Point Mixture with Flow Embedding Layer
The flow embedding layers predicts correspoinding points between two point cloud frames (frame $t$ and frame $t+1$) using a weighted decision. The network learns feature similarities and spatial repationships within these point pairs. The embedding layers take two inputs from point feature learning defined as follows:

$$\text{Input Points: } \lbrace p_{i} = (x_{i}, f_{i})_{i=1}^{n_{1}}\text{ and } q_{j} = (y_{j}, g_{j})_{j=1}^{n_{2}} \text{, } x_{i},y_{j}\in \mathbb{R^{3}} \text{, } f_{i},g_{j}\in \mathbb{R^{c}} \rbrace$$

In the first step, the flow embedding layer learns a flow embedding information for each point in the first frame defined as $ \lbrace e_{i} \rbrace _{i=1}^{n_{1}} \text{, where } e_{i}\in \mathbb{R^{c^{'}}} $. Additionally, the original coordinates $x_{i}$ of the points in the first frame is passed and the final layer output is ${o_{i}=\lbrace(x_{i},e_{i})}\rbrace$

$$ e_{i} =  \underset{\{j | \lVert x_{i} - x_{j}^{'}\rVert \le r\}}{MAX} \{h(f_{i}, g_{j}, y_{j} - x_{i}) \}$$

Compared to the set conv layer, the flow embedding layer uses given point $p_{i}$ in the first frame. Then the flow embedding layer finds all the points $q_{j}$ from the second frame in its radius neighborhood. If a particular point $q^{*}={y^{*},g^{*}}$ corresponds to p_{i}, then the flow of p_{i} were simply $y^{*} - x_{*}$. $h$ is a non-linear function with trainable parameters and MAX is an element-wise max pooling. Input two point features to h and learn to compute the weights. After this step, a few more set conv layers are used to obtain spatial smoothness.

![FlowNet3D](/images/blogs/flownet3d/flownet3d_flow_embedding_layer.png)
*Figure from [FlowNet3D: Learning Scene Flow in 3D Point Clouds](https://arxiv.org/pdf/1806.01411.pdf) by Liu et al.*
<br /><br />


### Flow Refinement with Set Upconv Layer

![FlowNet3D](/images/blogs/flownet3d/flownet3d_diagram.png)
*Figure from [FlowNet3D: Learning Scene Flow in 3D Point Clouds](https://arxiv.org/pdf/1806.01411.pdf) by Liu et al.*
<br /><br />

![FlowNet3D](/images/blogs/flownet3d/flownet3d_point_cloud_processing.png)
*Figure from [FlowNet3D: Learning Scene Flow in 3D Point Clouds](https://arxiv.org/pdf/1806.01411.pdf) by Liu et al.*
<br /><br />

learn hierarchical features of point clouds and flow embeddings that represent their motions

associate points from their spatial localities and geometric similarities -> flow embedding layers


## Theoretical Analysis

## Conclusion